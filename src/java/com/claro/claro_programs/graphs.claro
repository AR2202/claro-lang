# TODO(steving) 1. Try supporting "keyword generics" over `blocking` annotation as proposed in
# TODO(steving)    https://blog.rust-lang.org/inside-rust/2022/07/27/keyword-generics.html. This would allow avoiding
# TODO(steving)    the duplication issue in functional code that utilizes first class function params.
# TODO(steving) 2. Support Graph Functions in the Interpreted backend.
# TODO(steving) 3. Try modelling the builtin `input()` function as an async function to queue up interactions with the
# TODO(steving)    console so that graph functions can naturally make use of user input without different threads
# TODO(steving)    stepping all over each other. It also seems more naturally correct for input() to be explicitly
# TODO(steving)    modelled this way anyways since you are consciously waiting on the user for something.
# TODO(steving) 4. If you're able to model builtin `input()` as an async function, then consider extending this logic
# TODO(steving)    a generalized notion of a "SynchronizingResource" so that writes to a shared resource e.g. stdout via
# TODO(steving)    the builtin `print()` and any other following operations are guaranteed to happen as a logical
# TODO(steving)    synchronized unit. This would basically involve keeping a queue of futures to chain off of one
# TODO(steving)    another, where each new call into the SynchronizingResource is applied as a new future chained onto
# TODO(steving)    the last future. Then subsequent calls into the SynchronizingResource would be chained onto the new
# TODO(steving)    last future. All operations must either be a synchronized read, or a synchronized read and write, so
# TODO(steving)    operations passed to mutate should be a lambda of type function<UnwrappedType, UnwrappedType>. Going
# TODO(steving)    based on Java names for this thing, SynchronizedResource == Atomic* except in Claro this is non-blocking.

####################################################################################################
# DEMONSTRATE BASIC GRAPH FUNCTION SEMANTICS
####################################################################################################

# Compute ((3x+1)(x^2 - y))/(x^2) concurrently....which is super ridiculous but demonstrates a point:
#
#
#   times3:(3 * x)                        squared:(x ^ 2)
#       \                                      /      \
#        v                                    v       |
#     plus1:(1 + @times3)    minusY:(@squared - y)    /
#           \                         /              /
#            v                       v              /
#           multiply:(@plus1 * @minusY)            /
#                            \                    /
#                             v                  v
#                         divide:(@multiply / @squared)
#                                     |
#                                     v
#                                   result
graph function firstGraph(x: float, y: float) -> future<float> {
  root result <- @multiply / @squared;

  node multiply <- @plus1 * @minusY;

  node plus1 <- 1 + @times3;

  node minusY <- @squared - y;

  node times3 <- 3 * x;

  node squared <- x * x;
}

# The token <-| is referred to as the "(blocking) unwrap operator". I'm very intentionally not allowing more concise
# ways to unwrap a future (for example w/o a declaration to a variable) because I want blocking to be extremely
# apparent and I don't ever want it to be overlooked as blocking threads can frequently be a bad idea.
var firstConcurrentClaroResult <-| firstGraph(10.0, 2.0);
print(firstConcurrentClaroResult);

####################################################################################################
# DEMONSTRATE BASIC GRAPH PROVIDER SEMANTICS
####################################################################################################
graph provider firstGraphProvider() -> future<float> {
  root providedRes <- @deferredFirstGraphRes;
  node deferredFirstGraphRes <- firstGraph(10.0, 2.0);
}

var firstGraphProviderResult <-| firstGraphProvider();
print(firstGraphProviderResult);

####################################################################################################
# DEMONSTRATE BASIC GRAPH CONSUMER SEMANTICS - (FIRE-AND-FORGET)
####################################################################################################
#
# Note, graph consumer functions should really only be used rarely because you cannot guarantee
# ordering semantics relative to other async code because you are unable to retrieve a handle to the
# work in the form of a future<...> to schedule around. If Claro were to provide a handle, the type
# would have to be something like future<nothing>. But Claro is opposed to meaningless types like
# this so it would end up leading to the addition of a lot of complexity to the language to prevent
# binding any variables to a value of type `nothing` (since by definition that value would not exist).
#
# The killer use case for these existing is for the sake of a server running its infinite loop. The
# server should defer to `graph function` endpoint handlers to get the result, but then needs to be
# able to schedule work after the response is computed to serialize that response to the corresponding
# port and move on to scheduling handling of the next request.
#
# E.g. something like:
#
# consumer startServer(openPorts: [int]) {
#   while (true) {
#     var portsWithRequests: [Request] = getRequestsFromEpoll(openPorts);
#     # Map over a fire-and-forget async operation so the request thread never blocks.
#     map(portsWithRequests, handleRequest(port));
#   }
# }
#
# using(endpointHandler: function<Request -> Response>)
# graph consumer handleRequest(port: responsePort) {
#   root sendResponse <- serializeResponseToPort(port, @response);
#   node response <- endpointHandler(@request);
#   node request <- getRequestFromPort(port);
# }
#
# consumer serializeResponseToPort(port: int, response: Response) {
#   ... # This consumer function is simply going to do some system level IO, but it will have nothing to return.
# }
####################################################################################################

graph consumer graphReturningNothing(message: string) {
  root fireAndForget <- doIO(@formatted);
  node formatted <- "Formatted message: {message}";
}

consumer doIO(s: string) {
  print(s);
}

# Fire-and-forget the graph consumer procedure, it will finish at some point in the future.
var messages = ["execute first consumer graph!", "consumer graph again...", "one last time..."];
var i = 0;
while (i < len(messages)) {
  # There's no ordering guarantee for completion of all of the futures that I'm firing-and-forgetting here
  # since each call is potentially running concurrently in different threads. You'll likely need to run this
  # program many times to have a chance of observing that.
  graphReturningNothing(messages[i++]);
}

####################################################################################################
# DEMONSTRATE LAZY NODE PROVIDER INJECTION
####################################################################################################

graph function demoLazyNodeProviderInjection(x: int, y: int, shouldCallSubgraph: boolean) -> future<int> {
  root res <- @maybeX + @maybeY;
  node maybeX <- maybeCallSubgraph(@xSquared, 0, shouldCallSubgraph);
  node maybeY <- maybeCallSubgraph(@ySquared, 0, shouldCallSubgraph);
  node xSquared <- x * x;
  node ySquared <- y * y;
}

function maybeCallSubgraph(subgraph: provider<future<int>>, other: int, b: boolean) -> future<int> {
  if (b) {
    return subgraph();
  }
  return getIntFuture(other);
}

var callSubgraphRes <-| demoLazyNodeProviderInjection(2, 2, true);
print(callSubgraphRes);
print(callSubgraphRes == 8);
var doNotCallSubgraphRes <-| demoLazyNodeProviderInjection(2, 2, false);
print(doNotCallSubgraphRes);
print(doNotCallSubgraphRes == 0);

####################################################################################################
# DEMONSTRATE RECURSION OVER GRAPH FUNCTIONS & NON-BLOCKING VALIDATION
####################################################################################################
graph function nTimesRecursively(n: int, x: int, y: int) -> future<int> {
  root result <- 1 + @next1;

  # Claro prevents deadlocking by aggressively preventing blocking calls to be reachable from a graph function scope.
  # Try uncommenting one of these lines to see Claro complain at compile-time about the blocking call (even though it's
  # indirect, Claro will still correctly recognize it).
#  node next1 <- maybeRecurseBlockingIndirectly(n - 1 > 0, n, x, y);
#  node next1 <- maybeRecurseBlockingIndirectlyViaFirstClassFunctionDep(maybeRecurseBlockingIndirectly, n - 1 > 0, n, x, y);
  node next1 <- maybeRecurse(n - 1 > 0, n, x, y);
}

using(immediateFuture: function<int -> future<int>>)
function maybeRecurse(b: boolean, n: int, x: int, y: int) -> future<int> {
  if (b) {
    return nTimesRecursively(n - 1, x, y);
  }
  return immediateFuture(0);
}

# TODO(steving) I want to actually support keyword generics over the `blocking` color of this function because I want to
# TODO(steving) be deadlock-safe even when using higher-order functions, but I do not want to require these types of
# TODO(steving) higher-order functions to be implemented twice, once in each color.
#blocking:deferFn function maybeRecurseBlockingIndirectlyHidden(
blocking function maybeRecurseBlockingIndirectlyViaFirstClassFunctionDep(
#    deferFn: blocking? function<|boolean, int, int, int| -> int>,
    deferFn: blocking function<|boolean, int, int, int| -> int>,
    b: boolean,
    n: int,
    x: int,
    y: int) -> int {
  return deferFn(b, n, x, y);
}
# TODO(steving) Should be able to consolidate this func with the above via keyword generics.
function maybeRecurseNOTBlockingWithFirstClassFunctionDep(
    deferFn: function<|boolean, int, int, int| -> future<int>>,
    b: boolean,
    n: int,
    x: int,
    y: int) -> future<int> {
  return deferFn(b, n, x, y);
}

blocking function maybeRecurseBlockingIndirectly(b: boolean, n: int, x: int, y: int) -> int {
  return maybeRecurseBlocking(b, n, x, y);
}

blocking function maybeRecurseBlocking(b: boolean, n: int, x: int, y: int) -> int {
  if (b) {
    var res <-| nTimesRecursively(n - 1, x, y);
    return res;
  }
  return 0;
}

# TODO(steving) There's got to be an easier way to simply construct an immediate future. Try handling this the way we're
# TODO(steving) already planning to handle implicit auto-wrapping to optional<...> in function returns.
graph function getIntFuture(x: int) -> future<int> {
  root res <- @identity;
  node identity <- x;
}

module ImmediateFutureModule {
  bind immediateFuture:function<int -> future<int>> to getIntFuture;
}

using(ImmediateFutureModule) {
  var n = 200;
  var recursiveRes <-| nTimesRecursively(n, 10, 2);
  print(recursiveRes);
  print(recursiveRes == n);
}

var f: blocking function<|boolean, int, int, int| -> int> = maybeRecurseBlockingIndirectly;
print(f(false, -1, -1, -1));